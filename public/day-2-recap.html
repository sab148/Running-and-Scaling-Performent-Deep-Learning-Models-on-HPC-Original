<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Alexandre Strube // Sabrina Benassou // Ismail Khalfaoui">
  <title>Bringing Deep Learning Workloads to JSC supercomputers</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="./dist/reset.css">
  <link rel="stylesheet" href="./dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="./dist/theme/sky.css" id="theme">
  <style>
  .container{
    display: flex;
  }
  .col {
    flex: 1;
  }

  .slides {
      font-size: 0.75em;
  }
  .reveal ul {
      display: block;
  }
  .reveal ol {
      display: block;
  }

  img {
      max-height: 600px !important;
  }

  figcaption {
      font-size: 0.6em !important;
      font-style: italic !important;
  }

  .subtitle {
      font-style: italic !important;
  }

  .date {
      font-size: 0.75em !important;
  }


  body {
      font-family: "Arial", "sans-serif"
  }

  section {
      margin: 0;
  }

  .reveal .slides {
      margin: 0 1vmin;
  }
  .reveal h1,
  .reveal h2,
  .reveal h3,
  .reveal h4 {
      font-family: "Arial", "sans-serif";
      text-transform: Uppercase;
      color: #023d6b;
  }

  .reveal h1 {
      color: #023d6b;
      font-size: 250%;
  }


  .reveal h2 + h3 {
      text-transform: Unset;
      font-size: 80%;
  }

  .controls {
      visibility: hidden;
  }

  .reveal .progress {
      position: absolute;
      bottom: 1px;
  }

  .prompt {
      min-width: 0;
      width: 0;
      visibility: hidden;
  }

  div.dateauthor {
      padding-top: 4em;
      color: white;
  }

  div.prompt {
      width:0;
  }


  div#footer {
      position: fixed;
      bottom: 0;
      width: 100%;
      z-index: 10;
  font-size: 0.5em; font-weight: bold; padding: 0 1vmin; height: 20vmin; background: #fff}
  #footer h1 {
      position: absolute; 
      bottom: 3.2vmin; 
      display: block; 
      padding: 0 1em; 
      font-size: 1.7vmin;
      font-weight: bold;
      text-transform: unset;
      color: #023d6b;
  }
  #footer h2 {display: block; padding: 0.em 1em 0;}

  img.fzjlogo {
      position: fixed;
      bottom: 0;
      right: 0;
      height: 24vmin; /* The height of the svg is about 3 times the height of the logo */
      margin-bottom: -3vmin; /* Baseline of logo should be about 5% of short side above edge. */
  }

  .rendered_html img, svg {
      max-height: 440px;
  }

  </style>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Bringing Deep Learning Workloads to JSC
supercomputers</h1>
  <p class="subtitle">Recap of Day 2</p>
  <p class="author">Alexandre Strube // Sabrina Benassou // Ismail
Khalfaoui</p>
  <p class="date">September 17th, 2025</p>
</section>

<section>
<section id="summary-of-day-2" class="title-slide slide level1">
<h1>Summary of Day 2</h1>

</section>
<section id="quick-recap" class="slide level2">
<h2>Quick recap</h2>
<p>The team discussed data loading strategies and storage approaches for
supercomputers, including recommendations for code and data
organization, along with demonstrations of different data loading
methods and formats. Ismail presented on parallelization techniques
using Slurm and distributed training with PyTorch, covering topics like
transformer models, DDP implementation, and monitoring tools like
LLView. The team addressed various technical challenges including
PyTorch version compatibility issues, GPU utilization problems, and FSDP
implementation, while also discussing different parallelism approaches
for training large models.</p>
</section>
<section id="next-steps" class="slide level2">
<h2>Next steps</h2>
<p>Attendees: Store code in Project 1 of their respective projects
Attendees: Store data in Scratch for faster access and more memory
Attendees: Join DataOne datasets project for permanent data storage
using the provided link Attendees: Clone the provided GitHub repository
to access code examples for data loading strategies Attendees: Run the
data loading examples using the provided SBATCH files to test HDF5 and
Apache Arrow implementations Attendees: Change the reservation in SBatch
files from day one to day two to run the loader examples successfully
Attendees: Update the data path in their code to fix loading errors
Attendees: Access the pre-created ImageNet dataset files in the Scratch
directory to avoid recreating them Attendees: Check the pre-created HDF5
and Arrow files stored in the scratch/Data folder Attendees: Use RAM for
datasets smaller than 500 gigabytes Attendees: Consider using HDF5,
Apache Arrow, DALI, or SquashFS strategies for datasets larger than 500
gigabytes Attendees: Consider using HDF5 for large datasets with many
small files due to its better documentation and established use in
science Attendees: Review the GitHub repository code examples for data
loading strategies Attendees: Review the HDF5 and Apache Arrow examples
provided in the repository under the “Code and Data Loading” folder
Attendees: Review the ImageNet loader examples for different data
loading approaches Summary</p>
</section>
<section id="supercomputer-data-loading-best-practices"
class="slide level2">
<h2>Supercomputer Data Loading Best Practices</h2>
<p>Sabrina presented on data loading strategies for supercomputers,
covering best practices for storing code and data. She recommended
storing code in Project 1 and data in Scratch (with a 90-day data
retention policy) or DataOne datasets for permanent storage. Sabrina
demonstrated different data loading approaches including RAM usage for
datasets under 500GB, and explained strategies for larger datasets using
HDF5 files and Apache Arrow, highlighting the importance of managing
inode limitations in the file system.</p>
</section>
<section id="data-formats-and-parallelization-discussion"
class="slide level2">
<h2>Data Formats and Parallelization Discussion</h2>
<p>The team discussed data format options for their project, with
Sabrina recommending HDF5 files as they are faster and more established
than alternatives like PyArrow or NumPy files. Santiago and Scherer
encountered issues running the SBatch scripts, with Scherer’s problem
being resolved by changing the reservation from day one to day two.
Ismail was scheduled to present on parallelization using Slurm, building
on Alexandre’s previous discussion about setting up minimal examples of
parallel computing.</p>
</section>
<section id="transformer-training-and-parallelization"
class="slide level2">
<h2>Transformer Training and Parallelization</h2>
<p>Ismail led a discussion on training a transformer model using data
from Wikitext2, focusing on parallelization techniques and the use of
PyTorch and Hugging Face’s datasets library. He explained the
architecture of transformers and outlined the steps for setting up and
running the code on the compute nodes, including the use of Slurm for
job management and DDP for distributed training. Sabrina provided
clarification on commenting out lines related to downloading datasets
due to the lack of internet access on compute nodes, and Ismail
emphasized the importance of running data-loading scripts on the login
node before proceeding with training.</p>
</section>
<section id="data-storage-and-training-challenges" class="slide level2">
<h2>Data Storage and Training Challenges</h2>
<p>Ismail explained the different data storage formats like Parquet,
row-based, and tree-based storage, noting that Parquet is preferred for
deep learning tasks but less efficient for shuffling data. The team
discussed issues with running code on the supercomputer, including GPU
availability on login nodes and the need to use SBATCH for distributed
training. Santiago encountered errors when running the distributed
training code, which Ismail attributed to path problems and missing
internet access on compute nodes, suggesting they check the error files
for more details.</p>
</section>
<section id="llview-monitoring-tool-demonstration" class="slide level2">
<h2>LLView Monitoring Tool Demonstration</h2>
<p>Ismail demonstrated the LLView monitoring tool for tracking job
performance on the Jureca DC supercomputer. He showed how to access the
tool, view real-time metrics including CPU and GPU usage, and generate
PDF reports of job statistics including core hours used. The group
discussed how to cancel jobs using the “scancel” command and observed
that current GPU utilization was only at 23% despite requesting 4 GPUs,
which Ismail explained was due to the code not properly distributing
across all available GPUs.</p>
</section>
<section id="neural-network-parallelization-techniques"
class="slide level2">
<h2>Neural Network Parallelization Techniques</h2>
<p>Ismail explained the principles of parallelizing code for training
neural networks using multiple GPUs, focusing on Distributed Data
Parallel (DDP) approach. He discussed key concepts including
communication operations like broadcast and reduce, terminology such as
world size and global ranks, and the strategy of distributing data while
copying the model to each GPU. Ismail emphasized that while DDP provides
optimal performance when the model fits GPU memory, it comes with memory
inefficiency challenges, which will be addressed in future discussions
with an alternative parallelization scheme.</p>
</section>
<section id="reproducibility-in-distributed-deep-learning"
class="slide level2">
<h2>Reproducibility in Distributed Deep Learning</h2>
<p>Ismail explained the challenges of reproducibility in computer
science and deep learning, particularly when using PyTorch’s DDP
(Distributed Data Parallel) for multi-GPU training. He detailed how to
set up deterministic behavior in PyTorch, including proper seeding and
configuration settings, to achieve higher reproducibility levels. The
team then walked through the process of setting up distributed training,
including configuring the master address, port selection, and using
Torch Run with specific backend settings for their supercomputer
environment. Ismail guided the implementation of distributed training
features in the code, including importing distributed utilities, setting
up process groups, creating distributed samplers, and modifying the
model to use DDP with FSDP.</p>
</section>
<section id="distributed-training-code-implementation"
class="slide level2">
<h2>Distributed Training Code Implementation</h2>
<p>Ismail explained the implementation of print0 and save0 methods in
distributed training to prevent repeated outputs and model weight
overwrites. He demonstrated how to modify the code to use these methods
and discussed the importance of shuffling data for training but not for
validation. The team successfully ran the modified distributed training
code on the supercomputer, with some members reporting around 80-86% GPU
utilization. Ismail advised starting with one node for debugging before
scaling up, and clarified that the local batch size of 128 is
distributed across 4 GPUs, resulting in a global batch size of 512.</p>
</section>
<section id="fsdp-training-method-overview" class="slide level2">
<h2>FSDP Training Method Overview</h2>
<p>Ismail explained the differences between DDP and FSDP (Fully Sharded
Data Parallel) parallelization methods for training neural networks. He
demonstrated how FSDP shards model layers across GPUs to save memory,
making it suitable for larger models that don’t fit in single GPU
memory. The team discussed implementing FSDP in their code, including
modifications needed for saving and printing model weights. Ismail
advised that while FSDP allows training of models with billions of
parameters, it requires sufficient GPU memory and high-bandwidth
communication networks like InfiniBand to be effective.</p>
</section>
<section id="pytorch-version-compatibility-issues" class="slide level2">
<h2>PyTorch Version Compatibility Issues</h2>
<p>The team discussed issues with PyTorch version compatibility and
errors related to FSDP (Fully Sharded Data Parallel) implementation.
Ismail and Alexandre identified that the current environment uses
PyTorch 2.5, but the exercises require version 2.6, leading to import
errors. The team discovered that updating requirements.txt to version
2.6 and reinstalling the environment using setup.sh resolved the initial
import issues. Alexandre provided a fix on Slack involving forced
installation of NVIDIA CUDNN CU12, which appeared to address the
remaining CUDA-related errors. The team confirmed that the updated
environment allowed the code to enter the training loop without
errors.</p>
</section>
<section id="parallel-training-techniques-for-large-models"
class="slide level2">
<h2>Parallel Training Techniques for Large Models</h2>
<p>Ismail explained different parallelism techniques for training large
models, including model parallelism, pipeline parallelism, and tensor
parallelism. He discussed when to use Fully Sharded Data Parallel (FSDP)
versus these other approaches, noting that FSDP works well for models up
to 512 GPUs but may suffer from communication issues beyond that scale.
Tak shared that he needs to train a model that requires four H100 GPUs
due to its size, with gradient tensors consuming the majority of memory.
The session concluded with Alexandre announcing a survey for course
participants and confirming the project deadline as September 30th.</p>
</section></section>
    </div>
  </div>

  <script src="./dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="./plugin/notes/notes.js"></script>
  <script src="./plugin/search/search.js"></script>
  <script src="./plugin/zoom/zoom.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'fade',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // reveal.js plugins
        plugins: [
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
